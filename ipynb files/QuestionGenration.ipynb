{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This codes depicts Fine tuning of T5 model for question genration task\n",
        "\n",
        "Input: Path of pdf for which questions need to be generate, and path where generated questions pdf needs tpo be stored\n",
        "\n",
        "output: Pdf file containg all the questions generated.\n",
        "\n",
        "Model: BhuvanGowda/t5-small-finetuned-QuestionGen\n",
        "\n",
        "Link to model: https://huggingface.co/BhuvanGowda/t5-small-finetuned-QuestionGen/commit/3886dc3777fd04c6578f19a1c7cb8cbe817d22b1\n",
        "\n",
        "For more details of fine tuning refer below!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UgTYlrMwD34p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context: link to fine tuning of T5 model\n",
        "\n",
        "link: https://colab.research.google.com/drive/1GkXtFCn1S-WQIjcIny9p4EkQj1ESCuh7?usp=sharing"
      ],
      "metadata": {
        "id": "UWs6OHgxD3nR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vObcY6dhcVkK",
        "outputId": "9f9fbb76-ca89-4c03-f1cb-96ecaee620ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.4)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.3 in /usr/local/lib/python3.10/dist-packages (from pymupdf) (1.24.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "#Insatall pymupdf & reportlab\n",
        "!pip install --upgrade pymupdf --upgrade reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeFWE7eFZfYW"
      },
      "outputs": [],
      "source": [
        "#Importing necessary Packages\n",
        "import pymupdf as py\n",
        "from transformers import pipeline\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyY1uanEZj4D"
      },
      "outputs": [],
      "source": [
        "#Function extract_text_from_pdf: Data pipeline for Question genration task & returns List of questions genrated\n",
        "def extract_text_from_pdf(pdf_path):#Input: Path of the pdf to be processed\n",
        "    text = \"\" #empty string to store extracted text\n",
        "    pdf_document = py.open(pdf_path)\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document[page_num]\n",
        "        text += page.get_text()\n",
        "    pdf_document.close()\n",
        "\n",
        "    #returning the extracted text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p76QXM1JZm4r"
      },
      "outputs": [],
      "source": [
        "#Function generate_questions: Data pipeline for Question genration task & returns List of questions genrated\n",
        "def generate_questions(text, model, tokenizer, max_length=512): #Input: text extracted, Model, Tokenizer, Maximum length of segment to be considered while genration.\n",
        "    question_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer) #Creating pipeline\n",
        "    generated_questions = set()  # Use a set to store unique questions\n",
        "\n",
        "    # Split text into segments\n",
        "    text_segments = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "    #Question Gentarion happens here\n",
        "    for segment in text_segments:\n",
        "        if segment.strip():  # Ensure segment is not empty\n",
        "            #Question genrated\n",
        "            questions = question_generator(segment, num_return_sequences=1) #num_return_sequences specifies the number of questions function has to generate\n",
        "            for q in questions:\n",
        "              #Adding the question genrated to the list of generated questions\n",
        "              generated_questions.add(q['generated_text'])\n",
        "\n",
        "    #Returning List containing the questions generated\n",
        "    return list(generated_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQripIvCZpYb"
      },
      "outputs": [],
      "source": [
        "#Function save_questions_to_pdf: Saves the genrated questions into a downloadable Pdf file\n",
        "def save_questions_to_pdf(questions, output_pdf_path): #input List of questions genrated & path where the pdf has to be saved\n",
        "    pdf = canvas.Canvas(output_pdf_path, pagesize=letter)\n",
        "    pdf.setFont(\"Helvetica\", 12)\n",
        "\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        try:\n",
        "            pdf.drawString(50, 750 - i * 20, f\"Question {i}: {question}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question {i}: {e}\")\n",
        "\n",
        "    pdf.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjbzMY6mZTBL",
        "outputId": "7ef37386-18e1-4b57-fac3-4c8b5564300f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Questions:\n",
            "Question 1: What is one of the most popular uses of BERT?\n",
            "Question 2: What is the best technique for BERT?\n",
            "Question 3: What is BERT doing?\n",
            "Question 4: What is the name of the tokenizer that BERT uses to make this step a breeze\n",
            "Question 5: What is the biggest advantage of BERT?\n",
            "Question 6: What is BERT's superpower?\n",
            "Question 7: What is a type of model that uses self-attention mechanisms?\n",
            "Question 8: BERT helps search engines understand the context of your s search queries better?\n",
            "Question 9: What is the name of the Transformers library?\n",
            "Question 10: What is the name of the most popular version of BERT?\n",
            "Question 11: What is the BERT Fine-Tuning Tutorial with PyTorch?\n",
            "Question 12: What is the name of the original paper on BERT?\n",
            "Question 13: What is the best way to get started with BERT?\n",
            "Question 14: What is a common trap when training any machine learning model?\n",
            "Question 15: What is BERT?\n",
            "Question 16: What is the name of the special tokens that BERT loves?\n",
            "Question 17: What is a good tool to fine-tune?\n",
            "Question 18: What is the name of the mechanism that BERT uses to weigh the importance of each token in\n",
            "Question 19: What is BERT essentially?\n",
            "Question 20: What is the name of the blog of Jay Alammar?\n",
            "Question 21: What is the course for BERT?\n",
            "Question 22: What is the learning rate of your BERT model?\n",
            "Question 23: What is the technique that helps avoid overfitting?\n",
            "Generated questions saved to: /content/generated_questions.pdf\n"
          ]
        }
      ],
      "source": [
        "#Main function\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Specify the path to your PDF file\n",
        "    pdf_path = r\"/content/Document.pdf\"\n",
        "\n",
        "    # Specify the output PDF file path\n",
        "    output_pdf_path = r\"/content/generated_questions.pdf\"\n",
        "\n",
        "    # Extract text from PDF\n",
        "    pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "\n",
        "\n",
        "    #initaializing Model and tokenizer\n",
        "    #Link to model: https://huggingface.co/BhuvanGowda/t5-small-finetuned-QuestionGen/commit/3886dc3777fd04c6578f19a1c7cb8cbe817d22b1\n",
        "\n",
        "    #Initializing Model from hugging face\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"BhuvanGowda/t5-small-finetuned-QuestionGen\")\n",
        "\n",
        "    #Initializing tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"BhuvanGowda/t5-small-finetuned-QuestionGen\")\n",
        "\n",
        "\n",
        "\n",
        "    if pdf_text: #checling for extracted text is not empty\n",
        "\n",
        "        # Generate questions from extracted text\n",
        "        generated_questions = generate_questions(text = pdf_text, model = model,tokenizer=tokenizer)\n",
        "\n",
        "        if generated_questions: #checling for Questions generated is not empty\n",
        "            #Printing Quesrions Generated\n",
        "            print(\"Generated Questions:\")\n",
        "            for i, question in enumerate(generated_questions, 1):\n",
        "                print(f\"Question {i}: {question}\")\n",
        "\n",
        "            # Save generated questions to a PDF file\n",
        "            save_questions_to_pdf(generated_questions, output_pdf_path)\n",
        "            print(f\"Generated questions saved to: {output_pdf_path}\")\n",
        "\n",
        "        else:\n",
        "            print(\"No questions generated.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to extract text from PDF.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4gUMcs0dbiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}